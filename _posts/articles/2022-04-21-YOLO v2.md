# 1. Introduction

[ [ hierarchical view of object classification → allows us to combine distinct datasets together ] & [joint training algorithm → allows us to train object detectors on both detection and classification data ] ]


→ [ [leverages labeled detection images → to learn to precisely localize objects ] & [uses classification images → to increase its vocabulary and robustness ] ]


→ [harness the large amount of classification data we already have and use it to expand the scope of current detection systems]


→ [ detection → would like to scale to level of object classification ]


→ ?

# 2. Better

[ Batch Normalization → leads to significatn improvements in convergence & eliminate the need for other forms of regularization and dropout]


& [High Resolution Classifier → For YOLOv2 we first fine tune the classification network at the full 448 × 448 resolution for 10 epochs on ImageNet → This gives the network time to adjust its filters to work better on higher resolution input]


& [ [eliminate one pooling layer → to make the output of the network’s convolutional layers higher resolution] & [shrink the network → to operate on 416 input images instead of 448×448.] → YOLO’s convolutional layers downsample the image by a factor of 32 so by using an input image of 416 we get an output feature map of 13 × 13 → odd number of locations in our feature map so there is a single center cell → it’s good to predict these objects instead of four locations that are all nearby → Convolutional With Anchor Boxes & not fully connectied layers]

& [ K-means clusturing → good priors(사전 분포) → we can make it easier for the network to learn to predict good detections.]


![image](https://user-images.githubusercontent.com/58837749/164583813-833e83dc-d21a-4b27-bcdb-0a53ca5231d6.png)


![image](https://user-images.githubusercontent.com/58837749/164573785-7993d94e-9f58-476c-8ac0-7c43632f106d.png)


기존 region proposal networks에서는, t_x, t_y의 value를 예측하고, (x, y)라는 center의 좌표는 다음과 같이 구해진다.


![image](https://user-images.githubusercontent.com/58837749/164574540-1579861b-83cd-4c5c-9af8-5b06d705088f.png)


이러한 방식으로 구할 때에는, t_x와 t_y값이 통제되지 않아, 예측된 박스가 어느 곳에서든 나타날 수 있고, 이러한 방식은 비효율적(시간이 오래 걸린다)이고 불안정성을 키운다.

YOLO v2에서는 YOLO의 접근 방식을 따르고, location of the grid cell에 대한 location coordinates를 예측한다.


& [ We use a logistic activation to constrain the network’s predictions to fall in this range → This bounds the ground truth to fall between 0 and 1 → predicts 5 bounding boxes at each cell and 5 coordinates for each bounding box, t_x, t_y, t_w, t_h and t_o]

예를 들어, 셀의 offset이 이미지의 좌측 상단 코너인 (c_x, c_y)이고. 바운딩 박스 분포가 p_w, p_h를 지녔다면, 예측은 다음에 상응한다.


![image](https://user-images.githubusercontent.com/58837749/164583945-95497495-f5e9-47a9-8fc2-0fe76148db28.png)



& [ turns the 26 × 26 × 512 feature map into a 13 × 13 × 2048(13 x 13 x 512 x 2 x 2) feature map, which can be concatenated with the original
features → by stacking adjacent features into different channels instead of spatial locations, → The passthrough layer concatenates the higher resolution features with the low resolution features ]

& [ 416 x 416 input images → downsamples by a factor of 32 → Multi-Scale Training({320, 352, ..., 608}) → This regime forces the network to learn to predict well across a variety of input dimensions → the same network can predict detections at different resolutions ]

![image](https://user-images.githubusercontent.com/58837749/164576088-dc676406-bc61-46fd-82a8-c7f5f4c5a13a.png)





→ simplify the network and then make the representation easier to learn.


→ focus mainly on improving recall and localization while maintaining classification accuracy.

