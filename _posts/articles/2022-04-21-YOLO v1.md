# 1. 초록

[[whole detection pipleine is a single network → in one evaluation ] & single neural network & full images ]

→ [ predict spatially separated bounding boxes & associated class probailities ]

→ object detection as a regression problem

→ YOLO

→ Object Detection

→ [ processes images in real-time at 45 frames estremely fast per second & make more localization errors ]


# 2. 도입

## 1. 기존
[ classifiers & evaluate it at various locations & scales in a test image ] → Object Detection


e.g.
Systems like deformable parts model(DPM)의 경우 슬라이딩 윈도우의 방식을 사용


More recent approaches like R-CNN use region proposal methods generate potential bounding boxes in an image

→ each individual component must be trained separately

→  run a classifier on these proposed boxes → post-process

→ [ refine the bounding boxes & eliminate duplicate detections & rescore the boxes based on other objects in the scene ]

→ complex

→ slow & hard to optimize

## 2. YOLO


process images pixels

→ [ bounding box coordinates & class probabilites ]

→ [ predict what objects are & where they are ]


![image](https://user-images.githubusercontent.com/58837749/164391707-e2827a8a-d49d-425d-867c-ddf7f3bbe41d.png)


[ FAST

& [ GLOBAL → implicitly encodes contextual information about class as well as their appearance ]

& Generalizable representations of objects  ]

→ YOLO

R-CNN의 경우 큰 context를보지 못 하기 때문에 background 패치 인식에서 실수가 있다고 한다.


# 3. Unified Detection

grid : region for header layer's final feature map

divides the input image into an S by S grid

→ [the center of an object falls into a grid cell → that grid cell is responsible for detecting that object ]

→ separate components of object detection → unified into single neural network → [ predict each bounding box & all bounding boxes across all classes for an image simultaneously & confidence scores for those boxes ]


confidence : Pr(Object) ∗ IOU(truth / pred)


즉, 교집합 / 합집합(predictd or truth)


Each bounding box cosists of 5 predictions : x, y, w, h, confidence(c)

(x,y) : predicted relative to the whole image → represent the center of box relative to the bounds of the grid cell

따라서,

(x,y) → IOU → confidnece(c)

Each grid cell also predicts C conditional class probabilites, Pr(Class_i | Object)(C 조건부 확률).

These probabilities are conditioned on the grid cell containing an object.
: object가 있는 grid cell가 필요조건이라는 뜻

We only predict one set of class probabilities per grid cell, regardless of the number of boxes B.
: B가 여러 개 있어도 각 grid cell 당 한 세트의 클래스 확률만 예측

test에서는 C 조건부 확률과 개별적 box confidence predictions를 곱한다.

![image](https://user-images.githubusercontent.com/58837749/164405995-5ff132b5-7a86-4508-85a9-2fa9732606de.png)

These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object → which gives us class-specific confidence scores for each box

![image](https://user-images.githubusercontent.com/58837749/164406701-34a2979d-9277-4c34-8f8e-d2ee6f741c08.png)


정리하자면, YOLO를 위해선 세 가지 조건이 필요하다.

divide image into an S by S grid

each grid cell predicts B bounding boxes : the number of boxes
each grid cell predicts confidence for those boxes : Pr(Class_i | Object)

each grid cell predicts C class probabilites

These predictions are encoded as an S * S * ( B * 5 + C) tensor


# 4. Network Design

initial convolutional layers of the network → extract features from the image

fully connected layers → predict the output probabilites and coordinates

![image](https://user-images.githubusercontent.com/58837749/164409007-ad9b79b0-bff6-46eb-a71a-c472678a1fa2.png)

[ 24개의 convolutional layers → feature map ] → [ 2 fully connected layers(leaky ReLU) → predicts both class probabilities & bounding box coordinates ] → [

two param(lambda_coord & lambda_noobj) → [increase the loss from bounding box coordinate predictions & decrease the loss from confidence predictions for boxes that don't contain objects ]

& predict the square root of the bounding box width and height instead of the width and height directly → Our error metric should reflect that small deviations in large boxes matter less than in small boxes

→ weighted SSE ]
하지만 GoogLeNet에 사용된 모듈 대신, 1 by 1 reduction layers를 사용한 후에 3 by 3 convolutional layers가 적용되는 방식을 채택하였다.

→ assign one predictor to be "responsible" for predicting an object based on which prediction has the highest current IOU with the ground truth → specialization between the bounding box predictor → get better at predicting

하기 식은 위에서 언급한 요소들을 최적화 한 loss function이다.

![image](https://user-images.githubusercontent.com/58837749/164415078-c306e78a-6b74-4bc0-8839-fcc773947534.png)

"1 obj i" denotes if object appears in cell i and "1 ij" denotes that the jth bounding box predictor in cell i is “responsible” for that prediction.
: i는 object가 있는 i번째 셀을 의미하고, j는prediction에 responsible한 셀 i에 있는 j번째 바운딩 박스


1번째 식 : i번째 그리드에서 j번째 바운딩 박스와 진짜 객체의 좌표값 loss의 제곱합을 전체 그리드를 구한 것에 람다(가중치)를 곱해준 것


2번째 식 : i번째 그리드에서 j번째 바운딩 박스와 진짜 객체의 width와height를 루트를 씌운 loss의 제곱합을 전체 그리드를 구한 것에 람다(가중치)를 곱해준 것


3번째 식 : i번째 그리드에서 j번째 바운딩 박스와 진짜 객체의 C probablities의 loss의 제곱합을 전체 그리드를 구한 것에 람다(가중치)를 곱해준 것


4번째 식 : 3번째 식과 다른 점은, noobj라는 것이고, 그에 따라 람다(가중치)를 곱해준다는 것이다.


5번째 식 : p(c)는 뭐지...?


이에 더하여,

dropout & extensive data augmentation → avoid overfitting


# 5. Inference

one network evaluation → training


# 6. Limitations of YOLO

each grid cell only predicts two boxes & can only have one class → strong spatial constraints on bounding box predictions → limits the number of nearby objects that our model can predict(새때와 같이 그룹지어 있는 작은 물체를 detect하기 어려워 함)

objects with new or unusual aspect ratios or configurations → hard to generalize

incorrect localizations → our loss function treats errors the same in small bounding boxes versus large bounding boxes
